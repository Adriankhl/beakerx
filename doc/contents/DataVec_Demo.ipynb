{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataVec \n",
    "\n",
    "[DataVec](https://tablesaw.tech/) is easy to add to the BeakerX Groovy kernel.\n",
    "ETL Library for Machine Learning - data pipelines, data munging and wrangling\n",
    "\n",
    "This notebook has some basic demos of how to use DataVec, including ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//workaround on wrongly working IVY\n",
    "%classpath add jar \"demoResources/machine_learning_lib/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.datavec.api.transform.schema.Schema\n",
    "\n",
    "\n",
    "inputDataSchema = new Schema.Builder()\n",
    "            //We can for convenience define multiple columns of the same type\n",
    "            .addColumnsString(\"DateString\", \"TimeString\")\n",
    "            //We can define different column types for different types of data:\n",
    "            .addColumnCategorical(\"State\", Arrays.asList(\"GA\",\"VA\",\"IL\",\"MO\",\"IN\",\"KY\",\"MS\",\"LA\",\"AL\",\"TN\",\"OH\",\"NC\",\"MD\",\"CA\",\"AZ\",\"FL\",\"IA\",\"MN\",\"KS\",\"TX\",\"OK\",\"AR\",\"NE\",\"WA\",\"WY\",\"CO\",\"ID\",\"SD\",\"PA\",\"MT\",\"NV\",\"NY\",\"DE\",\"NM\",\"ME\",\"ND\",\"SC\",\"WV\",\"MI\",\"WI\",\"NH\",\"CT\",\"MA\"))\n",
    "            .addColumnsInteger(\"State No\", \"Scale\", \"Injuries\", \"Fatalities\")\n",
    "            //Some columns have restrictions on the allowable values, that we consider valid:\n",
    "            .addColumnsDouble(\"Start Lat\", \"Start Lon\", \"Length\", \"Width\")\n",
    "            .build();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.datavec.api.transform.TransformProcess\n",
    "import org.datavec.api.transform.condition.ConditionOp\n",
    "import org.datavec.api.transform.condition.column.CategoricalColumnCondition\n",
    "import org.datavec.api.transform.filter.ConditionFilter\n",
    "\n",
    "transformProcess = new TransformProcess.Builder(inputDataSchema)\n",
    "  //Let's remove some column we don't need\n",
    "  .removeColumns(\"DateString\", \"TimeString\", \"State No\")\n",
    "  //Now, suppose we only want to analyze tornadoes involving NY, MI, IL, MA. Let's filter out\n",
    "  // everything except for those states.\n",
    "  //Here, we are applying a conditional filter. We remove all of the examples that match the condition\n",
    "  // The condition is \"State\" isn't one of {\"NY\", \"MI\", \"IL\", \"MA\"}\n",
    "  .filter(new ConditionFilter(\n",
    "                new CategoricalColumnCondition(\"State\", ConditionOp.NotInSet, new HashSet<>(Arrays.asList(\"NY\", \"WA\")))))\n",
    "  .build();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.datavec.api.transform.TransformProcess\n",
    "import org.datavec.local.transforms.TableRecords\n",
    "\n",
    "outputSchema = transformProcess.getFinalSchema();\n",
    "table = TableRecords.tableFromSchema(outputSchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For purpose of our example, we load data from CSV file and transform it using DataVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.datavec.api.records.reader.impl.csv.CSVRecordReader\n",
    "import org.datavec.api.split.FileSplit\n",
    "import org.datavec.api.records.reader.impl.transform.TransformProcessRecordReader\n",
    "import java.nio.file.Paths\n",
    "\n",
    "writable = []\n",
    "\n",
    "TransformProcessRecordReader tprr = new TransformProcessRecordReader(new CSVRecordReader(0,\",\"), transformProcess)\n",
    "tprr.initialize(new FileSplit(Paths.get(\"Tablesaw/tornadoes_2014.csv\").toFile()))\n",
    "\n",
    "// Extract filtered data (omitting null values)\n",
    "while (tprr.hasNext()) {\n",
    "    elem = tprr.next();\n",
    "    if (elem) {\n",
    "      writable.add(elem)\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Fill Table with extracted data\n",
    "for (int row=0; row<writable.size; row++) {\n",
    "    for (int col=0; col<outputSchema.numColumns(); col++) {\n",
    "        column = table.column(col);\n",
    "        column.addCell(\"\" + writable[row][col])\n",
    "    }\n",
    "}\n",
    "\n",
    "table.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of network which classify two groups of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.nio.file.Paths\n",
    "import org.datavec.api.records.reader.RecordReader\n",
    "import org.datavec.api.records.reader.impl.csv.CSVRecordReader\n",
    "import org.datavec.api.split.FileSplit\n",
    "import org.datavec.api.util.ClassPathResource\n",
    "import org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator\n",
    "import org.deeplearning4j.eval.Evaluation\n",
    "import org.deeplearning4j.nn.api.OptimizationAlgorithm\n",
    "import org.deeplearning4j.nn.conf.MultiLayerConfiguration\n",
    "import org.deeplearning4j.nn.conf.NeuralNetConfiguration\n",
    "import org.deeplearning4j.nn.conf.Updater\n",
    "import org.deeplearning4j.nn.conf.layers.DenseLayer\n",
    "import org.deeplearning4j.nn.conf.layers.OutputLayer\n",
    "import org.deeplearning4j.nn.multilayer.MultiLayerNetwork\n",
    "import org.deeplearning4j.nn.weights.WeightInit\n",
    "import org.deeplearning4j.optimize.listeners.ScoreIterationListener\n",
    "import org.nd4j.linalg.activations.Activation\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import org.nd4j.linalg.dataset.DataSet\n",
    "import org.nd4j.linalg.factory.Nd4j\n",
    "import org.nd4j.linalg.lossfunctions.LossFunctions.LossFunction\n",
    "import org.nd4j.linalg.dataset.api.iterator.DataSetIterator\n",
    "\n",
    "def seed = 123;\n",
    "def learningRate = 0.01;\n",
    "def batchSize = 50;\n",
    "def nEpochs = 30;\n",
    "\n",
    "def numInputs = 2;\n",
    "def numOutputs = 2;\n",
    "def numHiddenNodes = 20;\n",
    "\n",
    "def filenameTrain = Paths.get(\"demoResources/machine_learning/linear_data_train.csv\").toFile();\n",
    "def filenameTest = Paths.get(\"demoResources/machine_learning/linear_data_eval.csv\").toFile();\n",
    "\n",
    "    //Load the training data:\n",
    "rr = new CSVRecordReader();\n",
    "rr.initialize(new FileSplit(filenameTrain));\n",
    "trainIter = new RecordReaderDataSetIterator(rr,batchSize,0,2);\n",
    "\n",
    "//Load the test/evaluation data:\n",
    "rrTest = new CSVRecordReader();\n",
    "rrTest.initialize(new FileSplit(filenameTest));\n",
    "testIter = new RecordReaderDataSetIterator(rrTest,batchSize,0,2);\n",
    "\n",
    "conf = new NeuralNetConfiguration.Builder()\n",
    "     .seed(seed)\n",
    "     .iterations(1)\n",
    "     .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\n",
    "     .learningRate(learningRate)\n",
    "     .updater(Updater.NESTEROVS)     //To configure: .updater(new Nesterovs(0.9))\n",
    "     .list()\n",
    "     .layer(0, new DenseLayer.Builder().nIn(numInputs).nOut(numHiddenNodes)\n",
    "      .weightInit(WeightInit.XAVIER)\n",
    "        .activation(Activation.RELU)\n",
    "        .build())\n",
    "        .layer(1, new OutputLayer.Builder(LossFunction.NEGATIVELOGLIKELIHOOD)\n",
    "            .weightInit(WeightInit.XAVIER)\n",
    "            .activation(Activation.SOFTMAX).weightInit(WeightInit.XAVIER)\n",
    "            .nIn(numHiddenNodes).nOut(numOutputs).build())\n",
    "        .pretrain(false)\n",
    "        .backprop(true)\n",
    "        .build();\n",
    "\n",
    "model = new MultiLayerNetwork(conf);\n",
    "model.init();\n",
    "model.setListeners(new ScoreIterationListener(10));  //Print score every 10 parameter updates\n",
    "\n",
    "for(int n = 0; n < nEpochs; n++) {\n",
    " model.fit( trainIter );\n",
    "}\n",
    "\n",
    "print \"Evaluate model....\"\n",
    "eval = new Evaluation(numOutputs);\n",
    "\n",
    "while(testIter.hasNext()){\n",
    "  currentElement = testIter.next();\n",
    "  INDArray features = currentElement.getFeatureMatrix();\n",
    "  INDArray lables = currentElement.getLabels();\n",
    "  INDArray predicted = model.output(features,false);\n",
    "\n",
    "  eval.eval(lables, predicted);\n",
    "}\n",
    "\n",
    "//Print the evaluation statistics\n",
    "print eval.stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval.stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//TODO \n",
    "Print graph showing you how the multilayer perceptron (MLP) has classified the data in the example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Groovy",
   "language": "groovy",
   "name": "groovy"
  },
  "language_info": {
   "codemirror_mode": "groovy",
   "file_extension": ".groovy",
   "mimetype": "",
   "name": "Groovy",
   "nbconverter_exporter": "",
   "version": "2.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
