{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataVec \n",
    "\n",
    "The [DataVec](https://deeplearning4j.org/datavec) library from DL4J is easy to add to the BeakerX kernel, including displaying its tables with the BeakerX interactive table widget.  DataVec is an ETL Library for Machine Learning, including data pipelines, data munging, and wrangling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added jars: [junit-4.8.2.jar, annotations-2.0.1.jar, reflections-0.9.10.jar, guava-15.0.jar, neoitertools-1.0.0.jar, jackson-0.9.1.jar, nd4j-api-0.9.1.jar, javassist-3.19.0-GA.jar, opencsv-2.3.jar, javacpp-1.3.3.jar, findbugs-annotations-1.3.9-1.jar, commons-compress-1.8.1.jar, nd4j-common-0.9.1.jar, slf4j-api-1.7.7.jar, datavec-api-0.9.1.jar, nd4j-buffer-0.9.1.jar, freemarker-2.3.23.jar, commons-lang3-3.3.1.jar, stax2-api-3.1.4.jar, nd4j-context-0.9.1.jar, commons-math3-3.3.jar, joda-time-2.9.2.jar, lombok-1.16.10.jar, fastutil-6.5.7.jar, commons-io-2.4.jar, snakeyaml-1.12.jar, stream-2.7.0.jar]\n",
      "Added jars: [datavec-local-0.9.1.jar, datavec-dataframe-0.9.1.jar]\n",
      "Added jars: [leptonica-1.73-1.3-linux-armhf.jar, slf4j-api-1.7.12.jar, deeplearning4j-nn-0.9.1.jar, leptonica-1.73-1.3-android-x86.jar, opencv-3.2.0-1.3-android-x86.jar, hdf5-1.10.0-patch1-1.3-windows-x86.jar, hdf5-1.10.0-patch1-1.3.jar, deeplearning4j-core-0.9.1.jar, leptonica-1.73-1.3.jar, libfreenect2-0.2.0-1.3.jar, libfreenect-0.5.3-1.3.jar, datavec-data-image-0.9.1.jar, commons-math3-3.4.1.jar, opencv-platform-3.2.0-1.3.jar, nd4j-base64-0.9.1.jar, flycapture-2.9.3.43-1.3.jar, common-image-3.1.1.jar, deeplearning4j-ui-components-0.9.1.jar, hdf5-1.10.0-patch1-1.3-linux-x86.jar, hdf5-1.10.0-patch1-1.3-linux-x86_64.jar, opencv-3.2.0-1.3-linux-ppc64le.jar, opencv-3.2.0-1.3.jar, opencv-3.2.0-1.3-windows-x86.jar, leptonica-1.73-1.3-android-arm.jar, leptonica-1.73-1.3-linux-ppc64le.jar, flandmark-1.07-1.3.jar, nearestneighbor-core-0.9.1.jar, opencv-3.2.0-1.3-macosx-x86_64.jar, imageio-core-3.1.1.jar, imageio-tiff-3.1.1.jar, libdc1394-2.2.4-1.3.jar, leptonica-1.73-1.3-windows-x86_64.jar, opencv-3.2.0-1.3-linux-x86.jar, commons-compress-1.8.jar, artoolkitplus-2.3.1-1.3.jar, joda-time-2.2.jar, ffmpeg-3.2.1-1.3.jar, xz-1.5.jar, leptonica-1.73-1.3-linux-x86.jar, imageio-bmp-3.1.1.jar, opencv-3.2.0-1.3-linux-x86_64.jar, javacv-1.3.3.jar, hdf5-1.10.0-patch1-1.3-macosx-x86_64.jar, imageio-psd-3.1.1.jar, commons-codec-1.10.jar, leptonica-1.73-1.3-linux-x86_64.jar, hdf5-1.10.0-patch1-1.3-windows-x86_64.jar, leptonica-1.73-1.3-windows-x86.jar, leptonica-1.73-1.3-macosx-x86_64.jar, hdf5-1.10.0-patch1-1.3-linux-ppc64le.jar, common-lang-3.1.1.jar, imageio-jpeg-3.1.1.jar, opencv-3.2.0-1.3-android-arm.jar, leptonica-platform-1.73-1.3.jar, guava-20.0.jar, lombok-1.16.16.jar, commons-lang3-3.4.jar, hdf5-platform-1.10.0-patch1-1.3.jar, librealsense-1.9.6-1.3.jar, common-io-3.1.1.jar, nd4j-jackson-0.9.1.jar, opencv-3.2.0-1.3-windows-x86_64.jar, imageio-metadata-3.1.1.jar, commons-net-3.1.jar, videoinput-0.200-1.3.jar, opencv-3.2.0-1.3-linux-armhf.jar, jai-imageio-core-1.3.0.jar, deeplearning4j-modelimport-0.9.1.jar]\n",
      "Added jars: [nd4j-native-api-0.9.1.jar, nd4j-native-0.9.1-linux-ppc64le.jar, openblas-0.2.19-1.3.jar, openblas-0.2.19-1.3-android-x86.jar, nd4j-native-0.9.1-windows-x86_64.jar, openblas-0.2.19-1.3-linux-armhf.jar, nd4j-native-0.9.1-android-arm.jar, openblas-0.2.19-1.3-linux-x86_64.jar, openblas-0.2.19-1.3-linux-ppc64le.jar, nd4j-native-0.9.1-macosx-x86_64.jar, slf4j-api-1.7.10.jar, openblas-platform-0.2.19-1.3.jar, nd4j-native-0.9.1-android-x86.jar, nd4j-native-0.9.1.jar, openblas-0.2.19-1.3-windows-x86_64.jar, openblas-0.2.19-1.3-linux-x86.jar, openblas-0.2.19-1.3-windows-x86.jar, openblas-0.2.19-1.3-macosx-x86_64.jar, openblas-0.2.19-1.3-android-arm.jar, nd4j-native-0.9.1-linux-x86_64.jar, nd4j-native-platform-0.9.1.jar]\n"
     ]
    }
   ],
   "source": [
    "%classpath add mvn org.datavec datavec-api 0.9.1\n",
    "%classpath add mvn org.datavec datavec-local 0.9.1\n",
    "%classpath add mvn org.datavec datavec-dataframe 0.9.1\n",
    "%classpath add mvn org.deeplearning4j deeplearning4j-core 0.9.1\n",
    "%classpath add mvn org.nd4j nd4j-native-platform 0.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%import org.nd4j.linalg.api.ndarray.INDArray\n",
    "%import org.datavec.api.split.FileSplit\n",
    "%import org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator\n",
    "%import java.nio.file.Paths\n",
    "%import org.nd4j.linalg.factory.Nd4j\n",
    "%import org.datavec.api.transform.TransformProcess\n",
    "%import org.datavec.api.records.reader.impl.csv.CSVRecordReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Schema():\n",
       "idx   name              type           meta data\n",
       "0     \"DateString\"      String         StringMetaData(name=\"DateString\",)\n",
       "1     \"TimeString\"      String         StringMetaData(name=\"TimeString\",)\n",
       "2     \"State\"           Categorical    CategoricalMetaData(name=\"State\",stateNames=[\"DE\",\"TX\",\"FL\",\"NV\",\"WA\",\"NY\",\"SC\",\"SD\",\"WI\",\"MA\",\"MD\",\"IA\",\"ME\",\"OH\",\"GA\",\"ID\",\"OK\",\"MI\",\"CA\",\"WV\",\"MN\",\"MO\",\"WY\",\"IL\",\"IN\",\"MS\",\"MT\",\"KS\",\"VA\",\"AL\",\"CO\",\"KY\",\"AR\",\"PA\",\"CT\",\"LA\",\"NC\",\"ND\",\"NE\",\"AZ\",\"TN\",\"NH\",\"NM\"])\n",
       "3     \"State No\"        Integer        IntegerMetaData(name=\"State No\",)\n",
       "4     \"Scale\"           Integer        IntegerMetaData(name=\"Scale\",)\n",
       "5     \"Injuries\"        Integer        IntegerMetaData(name=\"Injuries\",)\n",
       "6     \"Fatalities\"      Integer        IntegerMetaData(name=\"Fatalities\",)\n",
       "7     \"Start Lat\"       Double         DoubleMetaData(name=\"Start Lat\",allowNaN=false,allowInfinite=false)\n",
       "8     \"Start Lon\"       Double         DoubleMetaData(name=\"Start Lon\",allowNaN=false,allowInfinite=false)\n",
       "9     \"Length\"          Double         DoubleMetaData(name=\"Length\",allowNaN=false,allowInfinite=false)\n",
       "10    \"Width\"           Double         DoubleMetaData(name=\"Width\",allowNaN=false,allowInfinite=false)\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.datavec.api.transform.schema.Schema\n",
    "\n",
    "inputDataSchema = new Schema.Builder()\n",
    "            //We can for convenience define multiple columns of the same type\n",
    "            .addColumnsString(\"DateString\", \"TimeString\")\n",
    "            //We can define different column types for different types of data:\n",
    "            .addColumnCategorical(\"State\", Arrays.asList(\"GA\",\"VA\",\"IL\",\"MO\",\"IN\",\"KY\",\"MS\",\"LA\",\"AL\",\"TN\",\"OH\",\"NC\",\"MD\",\"CA\",\"AZ\",\"FL\",\"IA\",\"MN\",\"KS\",\"TX\",\"OK\",\"AR\",\"NE\",\"WA\",\"WY\",\"CO\",\"ID\",\"SD\",\"PA\",\"MT\",\"NV\",\"NY\",\"DE\",\"NM\",\"ME\",\"ND\",\"SC\",\"WV\",\"MI\",\"WI\",\"NH\",\"CT\",\"MA\"))\n",
    "            .addColumnsInteger(\"State No\", \"Scale\", \"Injuries\", \"Fatalities\")\n",
    "            //Some columns have restrictions on the allowable values, that we consider valid:\n",
    "            .addColumnsDouble(\"Start Lat\", \"Start Lon\", \"Length\", \"Width\")\n",
    "            .build();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformProcess(initialSchema=Schema():\n",
       "idx   name              type           meta data\n",
       "0     \"DateString\"      String         StringMetaData(name=\"DateString\",)\n",
       "1     \"TimeString\"      String         StringMetaData(name=\"TimeString\",)\n",
       "2     \"State\"           Categorical    CategoricalMetaData(name=\"State\",stateNames=[\"DE\",\"TX\",\"FL\",\"NV\",\"WA\",\"NY\",\"SC\",\"SD\",\"WI\",\"MA\",\"MD\",\"IA\",\"ME\",\"OH\",\"GA\",\"ID\",\"OK\",\"MI\",\"CA\",\"WV\",\"MN\",\"MO\",\"WY\",\"IL\",\"IN\",\"MS\",\"MT\",\"KS\",\"VA\",\"AL\",\"CO\",\"KY\",\"AR\",\"PA\",\"CT\",\"LA\",\"NC\",\"ND\",\"NE\",\"AZ\",\"TN\",\"NH\",\"NM\"])\n",
       "3     \"State No\"        Integer        IntegerMetaData(name=\"State No\",)\n",
       "4     \"Scale\"           Integer        IntegerMetaData(name=\"Scale\",)\n",
       "5     \"Injuries\"        Integer        IntegerMetaData(name=\"Injuries\",)\n",
       "6     \"Fatalities\"      Integer        IntegerMetaData(name=\"Fatalities\",)\n",
       "7     \"Start Lat\"       Double         DoubleMetaData(name=\"Start Lat\",allowNaN=false,allowInfinite=false)\n",
       "8     \"Start Lon\"       Double         DoubleMetaData(name=\"Start Lon\",allowNaN=false,allowInfinite=false)\n",
       "9     \"Length\"          Double         DoubleMetaData(name=\"Length\",allowNaN=false,allowInfinite=false)\n",
       "10    \"Width\"           Double         DoubleMetaData(name=\"Width\",allowNaN=false,allowInfinite=false)\n",
       ", actionList=[DataAction(RemoveColumnsTransform([DateString, TimeString, State No])), DataAction(ConditionFilter(CategoricalColumnCondition(columnName=\"State\",NotInSet,[WA, NY])))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.datavec.api.transform.condition.ConditionOp\n",
    "import org.datavec.api.transform.condition.column.CategoricalColumnCondition\n",
    "import org.datavec.api.transform.filter.ConditionFilter\n",
    "\n",
    "transformProcess = new TransformProcess.Builder(inputDataSchema)\n",
    "  //Let's remove some column we don't need\n",
    "  .removeColumns(\"DateString\", \"TimeString\", \"State No\")\n",
    "  //Now, suppose we only want to analyze tornadoes involving NY, MI, IL, MA. Let's filter out\n",
    "  // everything except for those states.\n",
    "  //Here, we are applying a conditional filter. We remove all of the examples that match the condition\n",
    "  // The condition is \"State\" isn't one of {\"NY\", \"MI\", \"IL\", \"MA\"}\n",
    "  .filter(new ConditionFilter(\n",
    "                new CategoricalColumnCondition(\"State\", ConditionOp.NotInSet, new HashSet<>(Arrays.asList(\"NY\", \"WA\")))))\n",
    "  .build();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda64ee4-3e61-4465-b7ba-7f1d151c6eec",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.datavec.local.transforms.TableRecords\n",
    "import jupyter.Displayer;\n",
    "import jupyter.Displayers;\n",
    "\n",
    "//JVM Repr to display table using our widget instead raw string table\n",
    "Displayers.register(org.datavec.dataframe.api.Table.class, new Displayer<org.datavec.dataframe.api.Table>() {\n",
    "      @Override\n",
    "      public Map<String, String> display(org.datavec.dataframe.api.Table table) {\n",
    "        return new HashMap<String, String>() {{\n",
    "          put(MIMEContainer.MIME.HIDDEN, \"\");\n",
    "          List<List<String>> values = new ArrayList<>();\n",
    "          for (int row=0; row<table.rowCount(); row++) {\n",
    "            List<String> rowValues = new ArrayList<>();\n",
    "            for (int column=0; column<table.columnCount(); column++) {              \n",
    "              rowValues.add(table.get(column, row));\n",
    "            }\n",
    "          values.add(rowValues);\n",
    "          }\n",
    "          System.out.println(values);  \n",
    "          tableDis = new TableDisplay(values, table.columnNames(), new ArrayList());\n",
    "          tableDis.display();\n",
    "        }};\n",
    "      }\n",
    "    });\n",
    "\n",
    "outputSchema = transformProcess.getFinalSchema()\n",
    "table = TableRecords.tableFromSchema(outputSchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For purpose of our example, we load data from CSV file and transform it using DataVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.datavec.api.records.reader.impl.transform.TransformProcessRecordReader\n",
    "\n",
    "writable = []\n",
    "\n",
    "TransformProcessRecordReader tprr = new TransformProcessRecordReader(new CSVRecordReader(0,\",\"), transformProcess)\n",
    "tprr.initialize(new FileSplit(Paths.get(\"../resources/data/tornadoes_2014.csv\").toFile()))\n",
    "\n",
    "// Extract filtered data (omitting null values)\n",
    "while (tprr.hasNext()) {\n",
    "    elem = tprr.next();\n",
    "    if (elem) {\n",
    "      writable.add(elem)\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82ef2df-c6ee-4cae-8d55-668b10d2adb8",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Fill Table with extracted data\n",
    "for (int row=0; row<writable.size; row++) {\n",
    "    for (int col=0; col<outputSchema.numColumns(); col++) {\n",
    "        column = table.column(col);\n",
    "        column.addCell(\"\" + writable[row][col])\n",
    "    }\n",
    "}\n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of network which classify two groups of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate model....\n",
      "Examples labeled as 0 classified by model as 0: 100 times\n",
      "Examples labeled as 1 classified by model as 1: 100 times\n",
      "\n",
      "\n",
      "==========================Scores========================================\n",
      " # of classes:    2\n",
      " Accuracy:        1.0000\n",
      " Precision:       1.0000\n",
      " Recall:          1.0000\n",
      " F1 Score:        1.0000\n",
      "========================================================================"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.datavec.api.records.reader.RecordReader\n",
    "import org.datavec.api.util.ClassPathResource\n",
    "import org.deeplearning4j.eval.Evaluation\n",
    "import org.deeplearning4j.nn.api.OptimizationAlgorithm\n",
    "import org.deeplearning4j.nn.conf.MultiLayerConfiguration\n",
    "import org.deeplearning4j.nn.conf.NeuralNetConfiguration\n",
    "import org.deeplearning4j.nn.conf.Updater\n",
    "import org.deeplearning4j.nn.conf.layers.DenseLayer\n",
    "import org.deeplearning4j.nn.conf.layers.OutputLayer\n",
    "import org.deeplearning4j.nn.multilayer.MultiLayerNetwork\n",
    "import org.deeplearning4j.nn.weights.WeightInit\n",
    "import org.deeplearning4j.optimize.listeners.ScoreIterationListener\n",
    "import org.nd4j.linalg.activations.Activation\n",
    "import org.nd4j.linalg.dataset.DataSet\n",
    "import org.nd4j.linalg.lossfunctions.LossFunctions.LossFunction\n",
    "import org.nd4j.linalg.dataset.api.iterator.DataSetIterator\n",
    "\n",
    "def seed = 123;\n",
    "def learningRate = 0.01;\n",
    "def batchSize = 50;\n",
    "def nEpochs = 30;\n",
    "\n",
    "def numInputs = 2;\n",
    "def numOutputs = 2;\n",
    "def numHiddenNodes = 20;\n",
    "\n",
    "def filenameTrain = Paths.get(\"../resources/data/linear_data_train.csv\").toFile();\n",
    "def filenameTest = Paths.get(\"../resources/data/linear_data_eval.csv\").toFile();\n",
    "\n",
    "    //Load the training data:\n",
    "rr = new CSVRecordReader();\n",
    "rr.initialize(new FileSplit(filenameTrain));\n",
    "trainIter = new RecordReaderDataSetIterator(rr,batchSize,0,2);\n",
    "\n",
    "//Load the test/evaluation data:\n",
    "rrTest = new CSVRecordReader();\n",
    "rrTest.initialize(new FileSplit(filenameTest));\n",
    "testIter = new RecordReaderDataSetIterator(rrTest,batchSize,0,2);\n",
    "\n",
    "conf = new NeuralNetConfiguration.Builder()\n",
    "  .seed(seed)\n",
    "  .iterations(1)\n",
    "  .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\n",
    "  .learningRate(learningRate)\n",
    "  .updater(Updater.NESTEROVS)     //To configure: .updater(new Nesterovs(0.9))\n",
    "  .list()\n",
    "  .layer(0, new DenseLayer.Builder().nIn(numInputs).nOut(numHiddenNodes)\n",
    "    .weightInit(WeightInit.XAVIER)\n",
    "    .activation(Activation.RELU)\n",
    "    .build())\n",
    "  .layer(1, new OutputLayer.Builder(LossFunction.NEGATIVELOGLIKELIHOOD)\n",
    "    .weightInit(WeightInit.XAVIER)\n",
    "    .activation(Activation.SOFTMAX).weightInit(WeightInit.XAVIER)\n",
    "    .nIn(numHiddenNodes).nOut(numOutputs).build())\n",
    "  .pretrain(false)\n",
    "  .backprop(true)\n",
    "  .build();\n",
    "\n",
    "model = new MultiLayerNetwork(conf);\n",
    "model.init();\n",
    "model.setListeners(new ScoreIterationListener(10));  //Print score every 10 parameter updates\n",
    "\n",
    "for(int n = 0; n < nEpochs; n++) {\n",
    " model.fit( trainIter );\n",
    "}\n",
    "\n",
    "print \"Evaluate model....\"\n",
    "eval = new Evaluation(numOutputs);\n",
    "\n",
    "while(testIter.hasNext()){\n",
    "  currentElement = testIter.next();\n",
    "  INDArray features = currentElement.getFeatureMatrix();\n",
    "  INDArray lables = currentElement.getLabels();\n",
    "  INDArray predicted = model.output(features,false);\n",
    "\n",
    "  eval.eval(lables, predicted);\n",
    "}\n",
    "\n",
    "//Print the evaluation statistics\n",
    "print eval.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c506881-f550-45dc-a100-dd032bf6848d",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.nd4j.linalg.api.ops.impl.indexaccum.IMax\n",
    "\n",
    "def filenameTrain = Paths.get(\"../resources/data/linear_data_train.csv\").toFile();\n",
    "def filenameTest = Paths.get(\"../resources/data/linear_data_eval.csv\").toFile();\n",
    "def extractDataFromND(features, labels) {\n",
    "    def classesOfPoints = [:]\n",
    "    nRows = features.rows()\n",
    "    nClasses = labels.columns()\n",
    "    INDArray argMax = Nd4j.getExecutioner().exec(new IMax(labels), 1);\n",
    "    \n",
    "    for (int i=0; i<features.rows(); i++) {\n",
    "        int classIdx = (int)argMax.getDouble(i);\n",
    "        classesOfPoints << [[features.getDouble(i, 0), features.getDouble(i, 1)]: classIdx]\n",
    "    }\n",
    "    \n",
    "    return classesOfPoints\n",
    "}\n",
    "\n",
    "rr.initialize(new FileSplit(filenameTrain))\n",
    "rr.reset()\n",
    "trainIter = new RecordReaderDataSetIterator(rr, 1000, 0, 2)\n",
    "ds = trainIter.next();\n",
    "rawTrainData = extractDataFromND(ds.getFeatures(), ds.getLabels())\n",
    "\n",
    "rrTest.initialize(new FileSplit(filenameTest))\n",
    "rrTest.reset();\n",
    "testIter = new RecordReaderDataSetIterator(rrTest,500,0,2);\n",
    "ds = testIter.next();\n",
    "INDArray testPredicted = model.output(ds.getFeatures());\n",
    "\n",
    "rawPredictedData = extractDataFromND(ds.getFeatures(), testPredicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9fb479b-8cae-4bf4-993a-6de38fc2e106",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot = new Plot(title: \"Training data Plot\")\n",
    "plot.setXBound([0.0, 1.0])\n",
    "plot.setYBound([-0.2, 0.8])\n",
    "\n",
    "rawTrainData.each{k, v -> \n",
    "    if (v==0) {\n",
    "       plot << new Points(x: [k[0]], y: [k[1]], color: Color.orange)\n",
    "    } else {\n",
    "       plot << new Points(x: [k[0]], y: [k[1]], color: Color.red)\n",
    "    }\n",
    "}\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8e8717-51bf-4990-8af2-d8951801b95d",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot = new Plot(title: \"Predicted data Plot\")\n",
    "plot.setXBound([0.0, 1.0])\n",
    "plot.setYBound([-0.2, 0.8])\n",
    "\n",
    "rawPredictedData.each{k, v -> \n",
    "    if (v==0) {\n",
    "       plot << new Points(x: [k[0]], y: [k[1]], color: Color.orange)\n",
    "    } else {\n",
    "       plot << new Points(x: [k[0]], y: [k[1]], color: Color.red)\n",
    "    }\n",
    "}\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Groovy",
   "language": "groovy",
   "name": "groovy"
  },
  "language_info": {
   "codemirror_mode": "groovy",
   "file_extension": ".groovy",
   "mimetype": "",
   "name": "Groovy",
   "nbconverter_exporter": "",
   "version": "2.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
